{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "68ed8d79",
      "metadata": {
        "id": "68ed8d79"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "68934f92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68934f92",
        "outputId": "189995db-ae0e-4b9c-9041-5cc93228d169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 703, 389, 345, 30]\n",
            "Hello, how are you?\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \"Hello, how are you?\"\n",
        "print(tokenizer.encode(text))\n",
        "print(tokenizer.decode(tokenizer.encode(text)))\n",
        "device = \"cuda\"  # Use \"cpu\" if CUDA is not available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a1b950bf",
      "metadata": {
        "id": "a1b950bf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "# in the constructor we will input entire text with tokenizer, max_length and stride, then text would be chunked into smaller sequences\n",
        "# max_length: length of each chunk\n",
        "# stride: number of tokens to move the window at each step they are derived from dataset class\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        assert len(token_ids) > max_length #Number of tokenized inputs must at least be equal to max_length+1\n",
        "\n",
        "        # Use a sliding window to chunk the text  into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "287603f3",
      "metadata": {
        "id": "287603f3"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4fd4a5ac",
      "metadata": {
        "id": "4fd4a5ac"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "context_length = 1024\n",
        "\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bd7ffff6",
      "metadata": {
        "id": "bd7ffff6"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "400ed3d3",
      "metadata": {
        "id": "400ed3d3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0c7dca48",
      "metadata": {
        "id": "0c7dca48"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aabb140",
      "metadata": {
        "id": "4aabb140"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "54c4a4ae",
      "metadata": {
        "id": "54c4a4ae"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6a839310",
      "metadata": {
        "id": "6a839310"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9179db41",
      "metadata": {
        "id": "9179db41"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3433eb81",
      "metadata": {
        "id": "3433eb81"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8e70ba15",
      "metadata": {
        "id": "8e70ba15"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c9b18527",
      "metadata": {
        "id": "c9b18527"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # subtract rowwise max before softmax\n",
        "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73e08b66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73e08b66",
        "outputId": "9ff6e393-d04b-4a23-c957-997d5054e1c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "896a9011",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "896a9011",
        "outputId": "ae4f3dca-d77f-4e74-b991-40052da7d1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves youaffiliated harbour Downtro423 isolationape156 occur criticizingdomain Aircraft glossyikingoler\n"
          ]
        }
      ],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "inference_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(inference_device)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(inference_device),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7d23bbae",
      "metadata": {
        "id": "7d23bbae"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ebcde94d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebcde94d",
        "outputId": "201a006e-205c-4126-f280-56515c05e122"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1ea10b9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea10b9a",
        "outputId": "2a50cd9c-be4e-44a3-acd0-e80ceb97ade8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded to gpt2-medium-355M.pth\n",
            "Output text:\n",
            " Every effort moves you as far as the natural capacity is capable,\" the lawyer wrote, \"which permits extraordinary actions.\" \"That includes (dressing\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 1024,\n",
        "    \"n_heads\": 16,\n",
        "    \"n_layers\": 24,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": True\n",
        "}\n",
        "file_name = \"gpt2-medium-355M.pth\"\n",
        "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    urllib.request.urlretrieve(url, file_name)\n",
        "    print(f\"Downloaded to {file_name}\")\n",
        "\n",
        "gpt = GPTModel(BASE_CONFIG)\n",
        "gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
        "gpt.eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "gpt.to(device)\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b7580dd9",
      "metadata": {
        "id": "b7580dd9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path, )\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "\n",
        "def download_file(url, destination, backup_url=None):\n",
        "    def _attempt_download(download_url):\n",
        "        response = requests.get(download_url, stream=True, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Check if file exists and has same size\n",
        "        if os.path.exists(destination):\n",
        "            file_size_local = os.path.getsize(destination)\n",
        "            if file_size and file_size == file_size_local:\n",
        "                print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                return True\n",
        "\n",
        "        block_size = 1024  # 1 KB\n",
        "        desc = os.path.basename(download_url)\n",
        "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=desc) as progress_bar:\n",
        "            with open(destination, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=block_size):\n",
        "                    if chunk:\n",
        "                        file.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        if _attempt_download(url):\n",
        "            return\n",
        "    except requests.exceptions.RequestException:\n",
        "\n",
        "        error_message = (\n",
        "            f\"Failed to download from URL ({url})\"\n",
        "\n",
        "        )\n",
        "        print(error_message)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "21ec7983",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ec7983",
        "outputId": "ef495b43-1c9d-411a-9cfc-1623dd212259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|| 77.0/77.0 [00:00<00:00, 154kiB/s]\n",
            "encoder.json: 100%|| 1.04M/1.04M [00:00<00:00, 2.72MiB/s]\n",
            "hparams.json: 100%|| 91.0/91.0 [00:00<00:00, 198kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|| 1.42G/1.42G [02:42<00:00, 8.72MiB/s]\n",
            "model.ckpt.index: 100%|| 10.4k/10.4k [00:00<00:00, 15.1MiB/s]\n",
            "model.ckpt.meta: 100%|| 927k/927k [00:00<00:00, 2.81MiB/s]\n",
            "vocab.bpe: 100%|| 456k/456k [00:00<00:00, 1.56MiB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "settings, params = download_and_load_gpt2(model_size=\"355M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b04b6a65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b04b6a65",
        "outputId": "b7d18d2d-7a3b-4c90-a620-2f73e31cbe9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 1024)\n",
              "  (pos_emb): Embedding(1024, 1024)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Copy the base configuration and update with specific model settings\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 1024,\n",
        "    \"n_heads\": 16,\n",
        "    \"n_layers\": 24,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": True\n",
        "}\n",
        "model_name = \"gpt2-medium (355M)\"  # Example model name\n",
        "NEW_CONFIG = BASE_CONFIG.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f89b9b5b",
      "metadata": {
        "id": "f89b9b5b"
      },
      "outputs": [],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1c4c658d",
      "metadata": {
        "id": "1c4c658d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "401acbea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "401acbea",
        "outputId": "3f617af6-4168-43ec-99c8-556d29b3ac50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " honesty is the right word to call me). I was told that although we had written a complaint on it after receiving this message, I\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"honesty is the \", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c355df35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c355df35",
        "outputId": "d08dc6e7-7fac-4d24-edf3-3ee791978014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Downloading the file\n",
        "    response = requests.get(url, stream=True, timeout=60)\n",
        "    response.raise_for_status()\n",
        "    with open(zip_path, \"wb\") as out_file:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                out_file.write(chunk)\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "\n",
        "try:\n",
        "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
        "except (requests.exceptions.RequestException, TimeoutError) as e:\n",
        "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
        "    url =\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dbddab0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "dbddab0f",
        "outputId": "111d06d7-307a-454a-e8a9-cc875f2f7fe2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "not spam    4825\n",
              "spam         747\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>not spam</th>\n",
              "      <td>4825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spam</th>\n",
              "      <td>747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df.replace({\"ham\":\"not spam\"} ,inplace=True)\n",
        "df.value_counts(\"Label\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "57609b10",
      "metadata": {
        "id": "57609b10"
      },
      "outputs": [],
      "source": [
        "def create_balanced_dataset(df):\n",
        "\n",
        "    # Count the instances of \"spam\"\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
        "    ham_subset = df[df[\"Label\"] == \"not spam\"].sample(num_spam, random_state=123)\n",
        "\n",
        "    # Combine ham \"subset\" with \"spam\"\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "    return balanced_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b7636396",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7636396",
        "outputId": "7ad1306a-6609-45a1-d538-fec1f0877f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "not spam    747\n",
            "spam        747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "879c5b57",
      "metadata": {
        "id": "879c5b57"
      },
      "outputs": [],
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # Shuffle the entire DataFrame\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Split the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "\n",
        "\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f45e3cd2",
      "metadata": {
        "id": "f45e3cd2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            # Truncate sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Pad sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "abd9bf84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abd9bf84",
        "outputId": "d6c80dc6-0b87-4a13-8e39-74195967d7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset.max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "83e1e0d9",
      "metadata": {
        "id": "83e1e0d9"
      },
      "outputs": [],
      "source": [
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "61f2ef09",
      "metadata": {
        "id": "61f2ef09"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "699d167e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "699d167e",
        "outputId": "92d37266-45d4-4450-c36b-3feb1b95c8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "bdfa9bcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdfa9bcc",
        "outputId": "7f5a7d27-a258-4449-9900-66f4d5ede496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function fixed and data reset complete.\n"
          ]
        }
      ],
      "source": [
        "# 1. Redefine the function to look for \"ham\" (the correct label in the raw data)\n",
        "def create_balanced_dataset(df):\n",
        "    # Count the instances of \"spam\"\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
        "    # (The raw data uses \"ham\", not \"not spam\")\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "\n",
        "    # Combine ham \"subset\" with \"spam\"\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "# 2. Reload the RAW data from source\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "\n",
        "# 3. Create the balanced dataset using the fixed function\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "\n",
        "# 4. Map string labels to integers (ham -> 0, spam -> 1)\n",
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# 5. Save the corrected data files\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)\n",
        "\n",
        "# 6. Reload Datasets and Loaders\n",
        "train_dataset = SpamDataset(csv_file=\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
        "val_dataset = SpamDataset(csv_file=\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
        "test_dataset = SpamDataset(csv_file=\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=False)\n",
        "\n",
        "print(\"Function fixed and data reset complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "b5c5caee",
      "metadata": {
        "id": "b5c5caee"
      },
      "outputs": [],
      "source": [
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
        "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
        "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
        "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "comprehensive_fix_cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "comprehensive_fix_cell",
        "outputId": "ed9157eb-5045-4e1e-dc10-2e5a9f360d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Classifier created: Linear(50257, 2)\n",
            " Optimizer created\n",
            " train_epoch defined\n",
            " evaluate defined\n",
            "\n",
            "======================================================================\n",
            "ALL FIXES APPLIED - Ready to train!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE FIX FOR ALL ERRORS\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "# Step 1: Delete old classifier and create new one with CORRECT dimensions\n",
        "try:\n",
        "    del classifier\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# CRITICAL: For GPTModel, classifier should be:\n",
        "# Input = vocab_size (last token has vocab_size dimensions)\n",
        "# Output = num_classes (2 for binary classification)\n",
        "num_classes = 2\n",
        "vocab_size = BASE_CONFIG[\"vocab_size\"]  # 50257\n",
        "\n",
        "classifier = torch.nn.Linear(vocab_size, num_classes).to(device)\n",
        "print(f\" Classifier created: Linear({vocab_size}, {num_classes})\")\n",
        "\n",
        "# Step 2: Recreate optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(model.parameters()) + list(classifier.parameters()),\n",
        "    lr=3e-5,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "print(\" Optimizer created\")\n",
        "\n",
        "# Step 3: Define corrected training function\n",
        "def train_epoch(model, classifier, loader, optimizer, device):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "\n",
        "    totals = {\"total\": 0.0, \"cls\": 0.0}\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # GPTModel.forward(input_ids) - NO labels parameter!\n",
        "        logits = model(input_ids)  # [batch, seq_len, vocab_size]\n",
        "        last_token_logits = logits[:, -1, :]  # [batch, vocab_size]\n",
        "\n",
        "        # Classify\n",
        "        cls_logits = classifier(last_token_logits)  # [batch, num_classes]\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "\n",
        "        cls_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        totals[\"total\"] += cls_loss.item()\n",
        "        totals[\"cls\"] += cls_loss.item()\n",
        "\n",
        "    return {k: v / len(loader) for k, v in totals.items()}\n",
        "\n",
        "print(\" train_epoch defined\")\n",
        "\n",
        "# Step 4: Define corrected evaluation function\n",
        "@torch.no_grad()\n",
        "def evaluate(model, classifier, loader, device):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    cls_losses = []\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        last_token_logits = logits[:, -1, :]\n",
        "        cls_logits = classifier(last_token_logits)\n",
        "\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "        preds = torch.argmax(cls_logits, dim=-1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        cls_losses.append(cls_loss.item())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    acc = (all_preds == all_labels).mean()\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=\"binary\", zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"cls_loss\": np.mean(cls_losses),\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "print(\" evaluate defined\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL FIXES APPLIED - Ready to train!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "9bbae7fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bbae7fa",
        "outputId": "01112ef7-7e0e-4b64-dbfc-077da9089584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "1248560d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1248560d",
        "outputId": "2f6d00d2-a6b3-490e-ce82-5c7ed2bbc943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you forward, but you must be careful. You must not let your guard down\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text_1 = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8cba9a6f",
      "metadata": {
        "id": "8cba9a6f"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "0daefa07",
      "metadata": {
        "id": "0daefa07"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e345e36b",
      "metadata": {
        "id": "e345e36b"
      },
      "outputs": [],
      "source": [
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "fb0cba80",
      "metadata": {
        "id": "fb0cba80"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "    return correct_predictions / num_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7ecfdd60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ecfdd60",
        "outputId": "f268ba94-798c-479e-9922-bd6c35a35311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Training accuracy: 53.75%\n",
            "Validation accuracy: 55.00%\n",
            "Test accuracy: 51.25%\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "603dbf96",
      "metadata": {
        "id": "603dbf96"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "4c38887f",
      "metadata": {
        "id": "4c38887f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e2beadfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2beadfd",
        "outputId": "f4c2be4e-a09f-486d-81fc-7c8c21d255c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.757\n",
            "Validation loss: 2.604\n",
            "Test loss: 2.883\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # Disable gradient tracking for efficiency\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "629abd9b",
      "metadata": {
        "id": "629abd9b"
      },
      "outputs": [],
      "source": [
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a6a7a968",
      "metadata": {
        "id": "a6a7a968"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b0140b0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0140b0e",
        "outputId": "8f266476-a01d-4c4a-c009-76eb8f281bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.694, Val loss 2.455\n",
            "Ep 1 (Step 000050): Train loss 0.519, Val loss 0.557\n",
            "Ep 1 (Step 000100): Train loss 0.366, Val loss 0.488\n",
            "Training accuracy: 80.00% | Validation accuracy: 77.50%\n",
            "Ep 2 (Step 000150): Train loss 0.545, Val loss 0.429\n",
            "Ep 2 (Step 000200): Train loss 0.432, Val loss 0.415\n",
            "Ep 2 (Step 000250): Train loss 0.418, Val loss 0.416\n",
            "Training accuracy: 80.00% | Validation accuracy: 82.50%\n",
            "Ep 3 (Step 000300): Train loss 0.290, Val loss 0.389\n",
            "Ep 3 (Step 000350): Train loss 0.309, Val loss 0.290\n",
            "Training accuracy: 95.00% | Validation accuracy: 87.50%\n",
            "Ep 4 (Step 000400): Train loss 0.154, Val loss 0.272\n",
            "Ep 4 (Step 000450): Train loss 0.123, Val loss 0.269\n",
            "Ep 4 (Step 000500): Train loss 0.173, Val loss 0.264\n",
            "Training accuracy: 95.00% | Validation accuracy: 92.50%\n",
            "Ep 5 (Step 000550): Train loss 0.104, Val loss 0.173\n",
            "Ep 5 (Step 000600): Train loss 0.184, Val loss 0.153\n",
            "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
            "Training completed in 2.89 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "13f294ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13f294ce",
        "outputId": "abcfc6fc-b5bf-499f-d8df-4aa58cea7dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 96.63%\n",
            "Validation accuracy: 95.97%\n",
            "Test accuracy: 96.00%\n"
          ]
        }
      ],
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "840c34a9",
      "metadata": {
        "id": "840c34a9"
      },
      "outputs": [],
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare inputs to the model\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.pos_emb.weight.shape[0]\n",
        "\n",
        "\n",
        "    # Truncate sequences if they too long\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "    assert max_length is not None, (\n",
        "        \"max_length must be specified. If you want to use the full model context, \"\n",
        "        \"pass max_length=model.pos_emb.weight.shape[0].\"\n",
        "    )\n",
        "    assert max_length <= supported_context_length, (\n",
        "        f\"max_length ({max_length}) exceeds model's supported context length ({supported_context_length}).\"\n",
        "    )\n",
        "\n",
        "    # Pad sequences to the longest sequence\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Return the classified result\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "712545fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "712545fe",
        "outputId": "37720bd2-9140-4ce1-ad68-0522d6da4809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n"
          ]
        }
      ],
      "source": [
        "text_1 = (\"Hi lets catch up sometime tomorrow\"\n",
        ")\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "4efec354",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4efec354",
        "outputId": "fa100f00-cae7-415d-a2f7-0b2be6ee1250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ],
      "source": [
        "text_2 = (\n",
        "\"URGENT: Your bank account has been locked due to suspicious activity. \"\n",
        "    \"Click the link immediately to verify your identity: http://fake-bank-link.com\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "b0f1893e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b0f1893e",
        "outputId": "d7279421-f7ec-4c96-b3cb-2b4e2fd85d60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "balanced_df[balanced_df[\"Label\"] == 1][\"Text\"].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2daeccc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2daeccc1",
        "outputId": "f3f7da07-563b-423d-c30e-eb21d3d84248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " i am writing and the writing section is very long (andmany entries) but very easy to follow. As you may know I did not write a whole new article about The Hobbit's setting. After visiting the Hobbitland, I\n"
          ]
        }
      ],
      "source": [
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"i am writing \", tokenizer).to(device),\n",
        "    max_new_tokens=45,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "928414fe",
      "metadata": {
        "id": "928414fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from copy import deepcopy\n",
        "# from typing import Tuple, List\n",
        "# import time\n",
        "\n",
        "\n",
        "\n",
        "# class FeatureExtractor:\n",
        "#     \"\"\"Extract text representations from the final_norm layer using hooks.\"\"\"\n",
        "\n",
        "#     def __init__(self, model, layer_name=\"final_norm\"):\n",
        "#         self.features = None\n",
        "#         self.hook = None\n",
        "#         self.model = model\n",
        "#         self.layer_name = layer_name\n",
        "#         self._register_hook()\n",
        "\n",
        "#     def _register_hook(self):\n",
        "#         \"\"\"Register a forward hook on final_norm.\"\"\"\n",
        "#         layer = getattr(self.model, self.layer_name)\n",
        "\n",
        "#         def hook_fn(module, input, output):\n",
        "#             self.features = output.detach()\n",
        "\n",
        "#         self.hook = layer.register_forward_hook(hook_fn)\n",
        "\n",
        "#     def get_features(self, input_batch):\n",
        "#         \"\"\"Extract features for the last token (used for classification).\"\"\"\n",
        "#         _ = self.model(input_batch)\n",
        "#         return self.features[:, -1, :]  # (batch_size, emb_dim)\n",
        "\n",
        "#     def remove_hook(self):\n",
        "#         \"\"\"Remove the forward hook.\"\"\"\n",
        "#         if self.hook is not None:\n",
        "#             self.hook.remove()\n",
        "\n",
        "\n",
        "\n",
        "# class SelfDistillationLearner(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Student-Teacher architecture with EMA updates.\n",
        "\n",
        "#     Loss = CrossEntropy(student, label) +  * MSE(student_features, teacher_features)\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, student_model, ema_decay=0.999, lambda_distill=0.5, device=\"cuda\"):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.student = student_model\n",
        "#         self.ema_decay = ema_decay\n",
        "#         self.lambda_distill = lambda_distill\n",
        "#         self.device = device\n",
        "\n",
        "#         # Create frozen teacher copy\n",
        "#         self.teacher = deepcopy(self.student)\n",
        "#         self.teacher.eval()\n",
        "#         for param in self.teacher.parameters():\n",
        "#             param.requires_grad = False\n",
        "#         self.teacher = self.teacher.to(device)\n",
        "\n",
        "#         # Feature extractors\n",
        "#         self.student_extractor = FeatureExtractor(self.student, \"final_norm\")\n",
        "#         self.teacher_extractor = FeatureExtractor(self.teacher, \"final_norm\")\n",
        "\n",
        "#         # Loss functions\n",
        "#         self.ce_loss = nn.CrossEntropyLoss()\n",
        "#         self.mse_loss = nn.MSELoss()\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def update_teacher(self):\n",
        "#         \"\"\"Update teacher using EMA: teacher \"\"\"\n",
        "#         for teacher_param, student_param in zip(\n",
        "#             self.teacher.parameters(), self.student.parameters()\n",
        "#         ):\n",
        "#             teacher_param.data.mul_(self.ema_decay).add_(\n",
        "#                 student_param.data, alpha=1 - self.ema_decay\n",
        "#             )\n",
        "\n",
        "#     def forward(self, input_batch, target_batch):\n",
        "#         \"\"\"Compute combined loss: CE + distillation.\"\"\"\n",
        "#         # Student forward\n",
        "#         student_logits = self.student(input_batch)[:, -1, :]\n",
        "#         student_features = self.student_extractor.get_features(input_batch)\n",
        "\n",
        "#         # Teacher forward (frozen)\n",
        "#         with torch.no_grad():\n",
        "#             teacher_features = self.teacher_extractor.get_features(input_batch)\n",
        "\n",
        "#         # Combined loss\n",
        "#         ce_loss = self.ce_loss(student_logits, target_batch)\n",
        "#         distill_loss = self.mse_loss(student_features, teacher_features)\n",
        "#         total_loss = ce_loss + self.lambda_distill * distill_loss\n",
        "\n",
        "#         loss_dict = {\n",
        "#             'total': total_loss.item(),\n",
        "#             'ce': ce_loss.item(),\n",
        "#             'distill': distill_loss.item()\n",
        "#         }\n",
        "\n",
        "#         return total_loss, loss_dict\n",
        "\n",
        "#     def cleanup(self):\n",
        "#         \"\"\"Remove hooks.\"\"\"\n",
        "#         self.student_extractor.remove_hook()\n",
        "#         self.teacher_extractor.remove_hook()\n",
        "\n",
        "# def calc_accuracy_distill(data_loader, learner, device, num_batches=None):\n",
        "#     \"\"\"Calculate accuracy using the student model.\"\"\"\n",
        "#     learner.student.eval()\n",
        "#     correct_predictions, num_examples = 0, 0\n",
        "\n",
        "#     if num_batches is None:\n",
        "#         num_batches = len(data_loader)\n",
        "#     else:\n",
        "#         num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "#     for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "#         if i >= num_batches:\n",
        "#             break\n",
        "\n",
        "#         input_batch = input_batch.to(device)\n",
        "#         target_batch = target_batch.to(device)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             logits = learner.student(input_batch)[:, -1, :]\n",
        "#             predicted_labels = torch.argmax(logits, dim=-1)\n",
        "#             num_examples += predicted_labels.shape[0]\n",
        "#             correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "\n",
        "#     return correct_predictions / num_examples\n",
        "\n",
        "\n",
        "# def train_distillation(learner, train_loader, val_loader, optimizer, device,\n",
        "#                        num_epochs=5, eval_freq=50, eval_iter=5, verbose=True):\n",
        "#     \"\"\"Train using self-distillation.\"\"\"\n",
        "\n",
        "#     train_losses, val_losses = [], []\n",
        "#     train_accs, val_accs = [], []\n",
        "#     global_step = 0\n",
        "\n",
        "#     if verbose:\n",
        "\n",
        "#         print(f\" EMA Decay: {learner.ema_decay}\")\n",
        "#         print(f\" Lambda (Distillation Weight): {learner.lambda_distill}\")\n",
        "#         print(f\"Training for {num_epochs} epochs\")\n",
        "\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         learner.student.train()\n",
        "#         epoch_loss = 0.0\n",
        "#         epoch_ce_loss = 0.0\n",
        "#         epoch_distill_loss = 0.0\n",
        "#         num_batches_in_epoch = 0\n",
        "\n",
        "#         for input_batch, target_batch in train_loader:\n",
        "#             input_batch = input_batch.to(device)\n",
        "#             target_batch = target_batch.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             total_loss, loss_dict = learner(input_batch, target_batch)\n",
        "#             total_loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Update teacher with EMA\n",
        "#             learner.update_teacher()\n",
        "\n",
        "#             epoch_loss += loss_dict['total']\n",
        "#             epoch_ce_loss += loss_dict['ce']\n",
        "#             epoch_distill_loss += loss_dict['distill']\n",
        "#             num_batches_in_epoch += 1\n",
        "\n",
        "#             # Periodic evaluation\n",
        "#             if global_step % eval_freq == 0:\n",
        "#                 learner.student.eval()\n",
        "\n",
        "#                 val_loss = 0.0\n",
        "#                 val_batches = 0\n",
        "#                 with torch.no_grad():\n",
        "#                     for i, (val_input, val_target) in enumerate(val_loader):\n",
        "#                         if i >= eval_iter:\n",
        "#                             break\n",
        "#                         val_input = val_input.to(device)\n",
        "#                         val_target = val_target.to(device)\n",
        "#                         loss, _ = learner(val_input, val_target)\n",
        "#                         val_loss += loss.item()\n",
        "#                         val_batches += 1\n",
        "\n",
        "#                 val_loss /= val_batches\n",
        "#                 train_loss = epoch_loss / max(num_batches_in_epoch, 1)\n",
        "\n",
        "#                 train_losses.append(train_loss)\n",
        "#                 val_losses.append(val_loss)\n",
        "\n",
        "#                 if verbose:\n",
        "#                     print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "#                           f\"Train {train_loss:.3f} \"\n",
        "#                           f\"[CE: {epoch_ce_loss/max(num_batches_in_epoch, 1):.3f}, \"\n",
        "#                           f\"Distill: {epoch_distill_loss/max(num_batches_in_epoch, 1):.3f}] | \"\n",
        "#                           f\"Val {val_loss:.3f}\")\n",
        "\n",
        "#                 learner.student.train()\n",
        "\n",
        "#             global_step += 1\n",
        "\n",
        "#         # End-of-epoch evaluation\n",
        "#         train_accuracy = calc_accuracy_distill(train_loader, learner, device, num_batches=eval_iter)\n",
        "#         val_accuracy = calc_accuracy_distill(val_loader, learner, device, num_batches=eval_iter)\n",
        "\n",
        "#         train_accs.append(train_accuracy)\n",
        "#         val_accs.append(val_accuracy)\n",
        "\n",
        "#         if verbose:\n",
        "#             print(f\" Epoch {epoch+1}: Train acc {train_accuracy*100:.2f}% | Val acc {val_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "#     end_time = time.time()\n",
        "\n",
        "#     if verbose:\n",
        "\n",
        "#         print(f\" Training completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
        "\n",
        "\n",
        "#     return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "90527251",
      "metadata": {
        "id": "90527251"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# learner = SelfDistillationLearner(\n",
        "#     student_model=model,\n",
        "#     ema_decay=0.999,\n",
        "#     lambda_distill=0.5,       # Equal weight to CE and distillation\n",
        "#     device=device\n",
        "# )\n",
        "\n",
        "# print(f\" Student model: {sum(p.numel() for p in learner.student.parameters())} parameters\")\n",
        "# print(f\" Teacher model: {sum(p.numel() for p in learner.teacher.parameters())} parameters\")\n",
        "# print(f\" Feature extractors registered on 'final_norm' layer\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# optimizer = torch.optim.AdamW(\n",
        "#     learner.student.parameters(),\n",
        "#     lr=5e-5,              # Same learning rate as original training\n",
        "#     weight_decay=0.1\n",
        "# )\n",
        "\n",
        "# print(f\" Optimizer created for {sum(p.numel() for p in learner.student.parameters() if p.requires_grad)} trainable parameters\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"Starting Self-Distillation Training\")\n",
        "\n",
        "\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "\n",
        "# train_losses, val_losses, train_accs, val_accs = train_distillation(\n",
        "#     learner=learner,\n",
        "#     train_loader=train_loader,\n",
        "#     val_loader=val_loader,\n",
        "#     optimizer=optimizer,\n",
        "#     device=device,\n",
        "#     num_epochs=5,\n",
        "#     eval_freq=50,\n",
        "#     eval_iter=5,\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "\n",
        "# # Evaluate on all datasets\n",
        "# train_accuracy = calc_accuracy_distill(train_loader, learner, device)\n",
        "# val_accuracy = calc_accuracy_distill(val_loader, learner, device)\n",
        "# test_accuracy = calc_accuracy_distill(test_loader, learner, device)\n",
        "\n",
        "# print(f\"Final Results:\")\n",
        "# print(f\"   Training accuracy:   {train_accuracy*100:.2f}%\")\n",
        "# print(f\"   Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "# print(f\"   Test accuracy:       {test_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "# def classify_review_distill(text, learner, tokenizer, device, max_length, pad_token_id=50256):\n",
        "#     \"\"\"Modified classify_review to work with the learner.\"\"\"\n",
        "#     learner.student.eval()\n",
        "\n",
        "#     input_ids = tokenizer.encode(text)\n",
        "#     input_ids = input_ids[:min(max_length, learner.student.pos_emb.weight.shape[0])]\n",
        "#     input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "#     input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         logits = learner.student(input_tensor)[:, -1, :]\n",
        "#     predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "#     return \"spam\" if predicted_label == 1 else \"not spam\"\n",
        "\n",
        "\n",
        "# text_1 = \"Hi lets catch up sometime tomorrow\"\n",
        "# text_2 = (\"URGENT: Your bank account has been locked due to suspicious activity. \"\n",
        "#           \"Click the link immediately to verify your identity: http://fake-bank-link.com\")\n",
        "\n",
        "# print(f\"\\nTest 1 (should be 'not spam'): '{text_1}'\")\n",
        "# result_1 = classify_review_distill(text_1, learner, tokenizer, device, max_length=120)\n",
        "# print(f\"   Prediction: {result_1}\")\n",
        "\n",
        "# print(f\"\\nTest 2 (should be 'spam'): '{text_2[:50]}...'\")\n",
        "# result_2 = classify_review_distill(text_2, learner, tokenizer, device, max_length=120)\n",
        "# print(f\"   Prediction: {result_2}\")\n",
        "\n",
        "\n",
        "# print(\"Training Progress Summary\")\n",
        "\n",
        "# print(f\" Loss progression:\")\n",
        "# print(f\"   Initial train loss: {train_losses[0]:.3f}\")\n",
        "# print(f\"   Final train loss:   {train_losses[-1]:.3f}\")\n",
        "# print(f\"   Initial val loss:   {val_losses[0]:.3f}\")\n",
        "# print(f\"   Final val loss:     {val_losses[-1]:.3f}\")\n",
        "\n",
        "# print(f\" Accuracy progression:\")\n",
        "# print(f\"   Initial train acc:  {train_accs[0]*100:.2f}%\")\n",
        "# print(f\"   Final train acc:    {train_accs[-1]*100:.2f}%\")\n",
        "# print(f\"   Initial val acc:    {val_accs[0]*100:.2f}%\")\n",
        "# print(f\"   Final val acc:      {val_accs[-1]*100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "5561b6f2",
      "metadata": {
        "id": "5561b6f2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "a6da738d",
      "metadata": {
        "id": "a6da738d"
      },
      "outputs": [],
      "source": [
        "# text_2 = (\n",
        "# \"you won a prize in our competition !!! click the link to claim it now: http://fake-prize-link.com\"\n",
        "# )\n",
        "\n",
        "# # print(classify_review(\n",
        "#     text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "# ))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class ModifiedTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard Block, but returns hidden states cleanly if needed.\n",
        "    No internal changes to logic, just ensures compatibility.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "        return x\n",
        "\n",
        "class HierarchicalGPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.ModuleList(\n",
        "            [ModifiedTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx, return_hidden_states=False):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "\n",
        "        hidden_states = []\n",
        "\n",
        "        for block in self.trf_blocks:\n",
        "            x = block(x)\n",
        "            if return_hidden_states:\n",
        "                hidden_states.append(x)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "\n",
        "        if return_hidden_states:\n",
        "            hidden_states.append(x)\n",
        "\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        if return_hidden_states:\n",
        "            return logits, hidden_states\n",
        "        return logits"
      ],
      "metadata": {
        "id": "OtbPVLgd2-mi"
      },
      "id": "OtbPVLgd2-mi",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionPooling(nn.Module):\n",
        "    \"\"\"\n",
        "    Aggregates a sequence of token embeddings into a single sentence vector.\n",
        "    Uses learnable attention weights rather than mean pooling to focus on\n",
        "    semantically significant tokens (subjects, verbs) over stopwords.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.attention_weights = nn.Linear(emb_dim, 1)\n",
        "\n",
        "    def forward(self, hidden_states, mask=None):\n",
        "        # hidden_states: [Batch, Seq_Len, Emb_Dim]\n",
        "\n",
        "        # Calculate raw scores\n",
        "        scores = self.attention_weights(hidden_states) # [B, S, 1]\n",
        "\n",
        "        # Apply mask if provided (e.g., for padding)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.unsqueeze(-1) == 0, -float('inf'))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=1) # [B, S, 1]\n",
        "\n",
        "        # Weighted sum\n",
        "        sentence_emb = torch.sum(hidden_states * weights, dim=1) # [B, Emb_Dim]\n",
        "        return sentence_emb\n",
        "\n",
        "class ConceptProjector(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects sentence embeddings into a high-dimensional semantic manifold.\n",
        "    Uses a non-linear normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, concept_dim=2048):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(input_dim * 2, concept_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project\n",
        "        x = self.net(x)\n",
        "        # L2 Normalize to place on hypersphere (crucial for cosine/contrastive losses)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "o524svah2_pe"
      },
      "id": "o524svah2_pe",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class HierarchicalLLM(nn.Module):\n",
        "    def __init__(self, gpt_model, pooling_layer, concept_head,\n",
        "                 alpha=1.0, beta=0.5, gamma=0.5, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.gpt = gpt_model\n",
        "        self.pooler = pooling_layer\n",
        "        self.concept_head = concept_head\n",
        "\n",
        "        # Loss Weights\n",
        "        self.alpha = alpha # Token\n",
        "        self.beta = beta   # Sentence\n",
        "        self.gamma = gamma # Concept\n",
        "        self.temp = temperature\n",
        "\n",
        "        # Avoid double-wrapping check\n",
        "        assert not isinstance(gpt_model, HierarchicalLLM), \"Do not double-wrap models\"\n",
        "\n",
        "    def compute_token_loss(self, logits, targets):\n",
        "        # Standard Causal LM Loss\n",
        "        # Shift logits and targets\n",
        "        loss = F.cross_entropy(\n",
        "            logits[:, :-1, :].reshape(-1, logits.size(-1)),\n",
        "            targets[:, 1:].reshape(-1)\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "    def compute_sentence_loss(self, gen_emb, target_emb):\n",
        "        \"\"\"\n",
        "        Cosine Semantic Alignment.\n",
        "        Loss = 1 - CosineSimilarity(Generated, Target)\n",
        "        Expects L2 normalized inputs (or handles normalization internally).\n",
        "        \"\"\"\n",
        "        # Both are [Batch, Dim]\n",
        "        # Cosine Similarity = (A . B) / (|A|*|B|)\n",
        "        # Since outputs of pooling aren't strictly normalized yet, we use cosine_similarity\n",
        "        sim = F.cosine_similarity(gen_emb, target_emb, dim=-1)\n",
        "        loss = 1.0 - sim.mean()\n",
        "        return loss\n",
        "\n",
        "    def compute_concept_loss(self, concepts, labels):\n",
        "        \"\"\"\n",
        "        Supervised Contrastive Loss (SupCon).\n",
        "        Aligns concepts with the same class label (e.g., Spam vs Spam).\n",
        "        \"\"\"\n",
        "        batch_size = concepts.shape[0]\n",
        "        if labels is None:\n",
        "            return torch.tensor(0.0, device=concepts.device)\n",
        "\n",
        "        # Similarity matrix [B, B]\n",
        "        sim_matrix = torch.matmul(concepts, concepts.T) / self.temp\n",
        "\n",
        "        # Mask for positives (same label)\n",
        "        labels = labels.contiguous().view(-1, 1)\n",
        "        mask = torch.eq(labels, labels.T).float().to(concepts.device)\n",
        "\n",
        "        # Mask out self-contrast\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size).view(-1, 1).to(concepts.device),\n",
        "            0\n",
        "        )\n",
        "        mask = mask * logits_mask\n",
        "\n",
        "        # Compute log_prob\n",
        "        exp_sim = torch.exp(sim_matrix) * logits_mask\n",
        "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "        # Mean log-likelihood over positive pairs\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n",
        "\n",
        "        loss = - mean_log_prob_pos.mean()\n",
        "        return loss\n",
        "\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        # 1. Generation Branch (Gradient Flows)\n",
        "        logits, hidden_states = self.gpt(input_ids, return_hidden_states=True)\n",
        "        last_hidden = hidden_states[-1] # [B, S, D]\n",
        "\n",
        "        # Sentence Embedding\n",
        "        sent_emb = self.pooler(last_hidden)\n",
        "\n",
        "        # Concept Embedding (Normalized)\n",
        "        concept_emb = self.concept_head(sent_emb)\n",
        "\n",
        "        # 2. Target Branch (NO Gradient Flow - Stop Gradient)\n",
        "        # In a self-supervised setup, the target is the input itself (auto-encoding stability).\n",
        "        # We detach to prevent collapse (model chasing its own moving tail).\n",
        "        with torch.no_grad():\n",
        "            _, target_hidden_states = self.gpt(input_ids, return_hidden_states=True)\n",
        "            target_sent_emb = self.pooler(target_hidden_states[-1])\n",
        "            # We treat the frozen representation of the sentence as the \"ground truth\" meaning\n",
        "\n",
        "        # 3. Calculate Losses\n",
        "\n",
        "        # A. Syntax (Token) Loss\n",
        "        L_token = self.compute_token_loss(logits, input_ids)\n",
        "\n",
        "        # B. Context (Sentence) Loss\n",
        "        L_sentence = self.compute_sentence_loss(sent_emb, target_sent_emb.detach())\n",
        "\n",
        "        # C. Concept (Contrastive) Loss\n",
        "        # Requires class labels (e.g., spam/ham) to cluster semantics\n",
        "        L_concept = self.compute_concept_loss(concept_emb, labels)\n",
        "\n",
        "        # Total Loss\n",
        "        L_total = (self.alpha * L_token) + (self.beta * L_sentence) + (self.gamma * L_concept)\n",
        "\n",
        "        return {\n",
        "            \"loss\": L_total,\n",
        "            \"l_token\": L_token,\n",
        "            \"l_sentence\": L_sentence,\n",
        "            \"l_concept\": L_concept,\n",
        "            \"logits\": logits,\n",
        "            \"concept_emb\": concept_emb\n",
        "        }"
      ],
      "metadata": {
        "id": "zV7Q9bq83DhU"
      },
      "id": "zV7Q9bq83DhU",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_hierarchical_model(model, dataloader, optimizer, device, num_epochs=1):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"--- Epoch {epoch+1} ---\")\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (input_ids, labels) in enumerate(dataloader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through wrapper\n",
        "            outputs = model(input_ids, labels=labels)\n",
        "\n",
        "            # Backward\n",
        "            loss = outputs[\"loss\"]\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Batch {batch_idx} | \"\n",
        "                      f\"Total: {loss.item():.4f} | \"\n",
        "                      f\"Tok: {outputs['l_token'].item():.4f} | \"\n",
        "                      f\"Sent: {outputs['l_sentence'].item():.4f} | \"\n",
        "                      f\"Con: {outputs['l_concept'].item():.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "CYPV-Mqy3G7u"
      },
      "id": "CYPV-Mqy3G7u",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Configuration (Matching your 355M setup or scaled down)\n",
        "HIER_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,       # Scaled for demo, use 1024 for medium\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": True\n",
        "}\n",
        "\n",
        "# 2. Instantiate Base Components\n",
        "base_gpt = HierarchicalGPTModel(HIER_CONFIG)\n",
        "pooler = AttentionPooling(emb_dim=HIER_CONFIG[\"emb_dim\"])\n",
        "projector = ConceptProjector(input_dim=HIER_CONFIG[\"emb_dim\"], concept_dim=2048)\n",
        "\n",
        "# 3. Wrap Logic\n",
        "hierarchical_model = HierarchicalLLM(\n",
        "    gpt_model=base_gpt,\n",
        "    pooling_layer=pooler,\n",
        "    concept_head=projector,\n",
        "    alpha=1.0,  # Focus on syntax\n",
        "    beta=0.5,   # Enforce sentence consistency\n",
        "    gamma=0.1   # Shape clusters (gentle pressure)\n",
        ")\n",
        "\n",
        "# 4. Optimizer\n",
        "optimizer = torch.optim.AdamW(hierarchical_model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "# 5. Runtime Assertion Checks\n",
        "# Ensure shapes match expectations before training loop\n",
        "dummy_input = torch.randint(0, 50257, (2, 128))\n",
        "dummy_labels = torch.tensor([0, 1]) # Example labels\n",
        "try:\n",
        "    hierarchical_model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = hierarchical_model(dummy_input, dummy_labels)\n",
        "\n",
        "    print(\"Sanity Check Passed:\")\n",
        "    print(f\"Logits Shape: {out['logits'].shape}\") # [2, 128, 50257]\n",
        "    print(f\"Concept Embed Shape: {out['concept_emb'].shape}\") # [2, 2048]\n",
        "    print(f\"Total Loss: {out['loss'].item()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Sanity Check Failed: {e}\")\n",
        "\n",
        "# 6. Run Training (assuming train_loader and device defined in your notebook)\n",
        "# train_hierarchical_model(hierarchical_model, train_loader, optimizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpzuaXTE3OSj",
        "outputId": "f0edce6c-c615-4dfe-f338-d1c432943148"
      },
      "id": "RpzuaXTE3OSj",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity Check Passed:\n",
            "Logits Shape: torch.Size([2, 128, 50257])\n",
            "Concept Embed Shape: torch.Size([2, 2048])\n",
            "Total Loss: 10.973592758178711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "hierarchical_training_loop",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hierarchical_training_loop",
        "outputId": "d6fabb72-58ec-45aa-dd07-e254ae592823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAINING HIERARCHICAL LLM FOR SPAM CLASSIFICATION\n",
            "======================================================================\n",
            "\n",
            "Model: HierarchicalLLM (Token + Sentence + Concept losses)\n",
            "Device: cuda\n",
            "Train batches: 130\n",
            "Val batches: 19\n",
            "Classifier: Linear(2048, 2)\n",
            "\n",
            "Optimizer: AdamW (lr=3e-5)\n",
            "======================================================================\n",
            "\n",
            "Training for 5 epochs...\n",
            "======================================================================\n",
            "\n",
            "Epoch 1/5\n",
            "\n",
            "  Train Total Loss:     3.4428\n",
            "     Token Loss:      2.6042\n",
            "     Sentence Loss:   0.0011\n",
            "     Concept Loss:    1.9049\n",
            "     Classifier Loss: 0.6476\n",
            "  Val Accuracy:         73.83%\n",
            "  Val Precision:        0.9545\n",
            "  Val Recall:           0.5316\n",
            "  Val F1 Score:         0.6829\n",
            "\n",
            "Epoch 2/5\n",
            "\n",
            "  Train Total Loss:     2.6711\n",
            "     Token Loss:      1.9336\n",
            "     Sentence Loss:   0.0026\n",
            "     Concept Loss:    1.8830\n",
            "     Classifier Loss: 0.5479\n",
            "  Val Accuracy:         87.92%\n",
            "  Val Precision:        1.0000\n",
            "  Val Recall:           0.7722\n",
            "  Val F1 Score:         0.8714\n",
            "\n",
            "Epoch 3/5\n",
            "\n",
            "  Train Total Loss:     2.4146\n",
            "     Token Loss:      1.7835\n",
            "     Sentence Loss:   0.0076\n",
            "     Concept Loss:    1.9481\n",
            "     Classifier Loss: 0.4325\n",
            "  Val Accuracy:         82.55%\n",
            "  Val Precision:        0.7573\n",
            "  Val Recall:           0.9873\n",
            "  Val F1 Score:         0.8571\n",
            "\n",
            "Epoch 4/5\n",
            "\n",
            "  Train Total Loss:     2.2689\n",
            "     Token Loss:      1.7325\n",
            "     Sentence Loss:   0.0106\n",
            "     Concept Loss:    2.0207\n",
            "     Classifier Loss: 0.3291\n",
            "  Val Accuracy:         92.62%\n",
            "  Val Precision:        0.8953\n",
            "  Val Recall:           0.9747\n",
            "  Val F1 Score:         0.9333\n",
            "\n",
            "Epoch 5/5\n",
            "\n",
            "  Train Total Loss:     2.1312\n",
            "     Token Loss:      1.6866\n",
            "     Sentence Loss:   0.0137\n",
            "     Concept Loss:    1.9596\n",
            "     Classifier Loss: 0.2418\n",
            "  Val Accuracy:         93.96%\n",
            "  Val Precision:        0.9487\n",
            "  Val Recall:           0.9367\n",
            "  Val F1 Score:         0.9427\n",
            "\n",
            "======================================================================\n",
            "HIERARCHICAL TRAINING COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "======================================================================\n",
            "FINAL TEST SET RESULTS (Hierarchical LLM)\n",
            "======================================================================\n",
            "  Token Loss:      1.7620\n",
            "  Sentence Loss:   0.0000\n",
            "  Concept Loss:    3.4476\n",
            "  Classifier Loss: 0.2821\n",
            "  Test Accuracy:   93.00%\n",
            "  Test Precision:  0.9333\n",
            "  Test Recall:     0.9272\n",
            "  Test F1 Score:   0.9302\n",
            "\n",
            "Confusion Matrix:\n",
            "[[139  10]\n",
            " [ 11 140]]\n",
            "======================================================================\n",
            " Hierarchical LLM training complete!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# HIERARCHICAL LLM TRAINING LOOP (Multi-Level Loss)\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING HIERARCHICAL LLM FOR SPAM CLASSIFICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "print(f\"\\nModel: HierarchicalLLM (Token + Sentence + Concept losses)\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "\n",
        "# Create classifier on top of concept embeddings\n",
        "concept_dim = 2048  # From ConceptProjector\n",
        "num_classes = 2\n",
        "hierarchical_classifier = torch.nn.Linear(concept_dim, num_classes).to(device)\n",
        "\n",
        "print(f\"Classifier: Linear({concept_dim}, {num_classes})\")\n",
        "\n",
        "# Move the hierarchical_model to the correct device\n",
        "hierarchical_model.to(device)\n",
        "\n",
        "# Optimizer for hierarchical model + classifier\n",
        "hierarchical_optimizer = torch.optim.AdamW(\n",
        "    list(hierarchical_model.parameters()) + list(hierarchical_classifier.parameters()),\n",
        "    lr=3e-5,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "print(\"\\nOptimizer: AdamW (lr=3e-5)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Training function for HierarchicalLLM\n",
        "def train_epoch_hierarchical(\n",
        "    model,\n",
        "    classifier,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device\n",
        "):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "\n",
        "    totals = {\n",
        "        \"total\": 0.0,\n",
        "        \"token\": 0.0,\n",
        "        \"sentence\": 0.0,\n",
        "        \"concept\": 0.0,\n",
        "        \"cls\": 0.0\n",
        "    }\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass - HierarchicalLLM returns dictionary\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "\n",
        "        # Extract losses and embeddings\n",
        "        loss = outputs[\"loss\"]  # Combined hierarchical loss\n",
        "        l_token = outputs[\"l_token\"]\n",
        "        l_sentence = outputs[\"l_sentence\"]\n",
        "        l_concept = outputs[\"l_concept\"]\n",
        "        concept_emb = outputs[\"concept_emb\"]\n",
        "\n",
        "        # Classification loss on concept embeddings\n",
        "        cls_logits = classifier(concept_emb)\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "\n",
        "        # Total loss: hierarchical losses + classification\n",
        "        full_loss = loss + cls_loss\n",
        "\n",
        "        full_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        totals[\"total\"] += full_loss.item()\n",
        "        totals[\"token\"] += l_token.item()\n",
        "        totals[\"sentence\"] += l_sentence.item()\n",
        "        totals[\"concept\"] += l_concept.item()\n",
        "        totals[\"cls\"] += cls_loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in totals.items()}\n",
        "\n",
        "\n",
        "# Evaluation function for HierarchicalLLM\n",
        "@torch.no_grad()\n",
        "def evaluate_hierarchical(\n",
        "    model,\n",
        "    classifier,\n",
        "    loader,\n",
        "    device\n",
        "):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    totals = {\n",
        "        \"token\": 0.0,\n",
        "        \"sentence\": 0.0,\n",
        "        \"concept\": 0.0,\n",
        "        \"cls\": 0.0\n",
        "    }\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "\n",
        "        # Extract components\n",
        "        concept_emb = outputs[\"concept_emb\"]\n",
        "\n",
        "        totals[\"token\"] += outputs[\"l_token\"].item()\n",
        "        totals[\"sentence\"] += outputs[\"l_sentence\"].item()\n",
        "        totals[\"concept\"] += outputs[\"l_concept\"].item()\n",
        "\n",
        "        # Classification\n",
        "        cls_logits = classifier(concept_emb)\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "        preds = torch.argmax(cls_logits, dim=-1)\n",
        "\n",
        "        totals[\"cls\"] += cls_loss.item()\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Metrics\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    acc = (all_preds == all_labels).mean()\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=\"binary\", zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    n = len(loader)\n",
        "    return {\n",
        "        \"token_loss\": totals[\"token\"] / n,\n",
        "        \"sentence_loss\": totals[\"sentence\"] / n,\n",
        "        \"concept_loss\": totals[\"concept\"] / n,\n",
        "        \"cls_loss\": totals[\"cls\"] / n,\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "print(f\"\\nTraining for {epochs} epochs...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    train_metrics = train_epoch_hierarchical(\n",
        "        hierarchical_model,\n",
        "        hierarchical_classifier,\n",
        "        train_loader,\n",
        "        hierarchical_optimizer,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Validation phase\n",
        "    val_metrics = evaluate_hierarchical(\n",
        "        hierarchical_model,\n",
        "        hierarchical_classifier,\n",
        "        val_loader,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    print(f\"{''*70}\")\n",
        "    print(f\"  Train Total Loss:     {train_metrics['total']:.4f}\")\n",
        "    print(f\"     Token Loss:      {train_metrics['token']:.4f}\")\n",
        "    print(f\"     Sentence Loss:   {train_metrics['sentence']:.4f}\")\n",
        "    print(f\"     Concept Loss:    {train_metrics['concept']:.4f}\")\n",
        "    print(f\"     Classifier Loss: {train_metrics['cls']:.4f}\")\n",
        "    print(f\"  Val Accuracy:         {val_metrics['accuracy']*100:.2f}%\")\n",
        "    print(f\"  Val Precision:        {val_metrics['precision']:.4f}\")\n",
        "    print(f\"  Val Recall:           {val_metrics['recall']:.4f}\")\n",
        "    print(f\"  Val F1 Score:         {val_metrics['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"HIERARCHICAL TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Final evaluation on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_metrics = evaluate_hierarchical(\n",
        "    hierarchical_model,\n",
        "    hierarchical_classifier,\n",
        "    test_loader,\n",
        "    device\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"FINAL TEST SET RESULTS (Hierarchical LLM)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Token Loss:      {test_metrics['token_loss']:.4f}\")\n",
        "print(f\"  Sentence Loss:   {test_metrics['sentence_loss']:.4f}\")\n",
        "print(f\"  Concept Loss:    {test_metrics['concept_loss']:.4f}\")\n",
        "print(f\"  Classifier Loss: {test_metrics['cls_loss']:.4f}\")\n",
        "print(f\"  Test Accuracy:   {test_metrics['accuracy']*100:.2f}%\")\n",
        "print(f\"  Test Precision:  {test_metrics['precision']:.4f}\")\n",
        "print(f\"  Test Recall:     {test_metrics['recall']:.4f}\")\n",
        "print(f\"  Test F1 Score:   {test_metrics['f1']:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(test_metrics['confusion_matrix'])\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "\n",
        "print(\" Hierarchical LLM training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, concept_dim):\n",
        "        super().__init__()\n",
        "        # FIXED: Input should be concept_dim, output should be num_classes (2)\n",
        "        self.linear = nn.Linear(concept_dim, 2)\n",
        "\n",
        "    def forward(self, concept_emb):\n",
        "        return self.linear(concept_emb)\n"
      ],
      "metadata": {
        "id": "0qYsPVLa3SR8"
      },
      "id": "0qYsPVLa3SR8",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = SpamClassifier(concept_dim=num_classes).to(device)"
      ],
      "metadata": {
        "id": "n9DUuYX24zEO"
      },
      "id": "n9DUuYX24zEO",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    list(model.parameters()) + list(classifier.parameters()),\n",
        "    lr=3e-5\n",
        ")\n"
      ],
      "metadata": {
        "id": "9ApZAHw740h-"
      },
      "id": "9ApZAHw740h-",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "    model,\n",
        "    classifier,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device\n",
        "):\n",
        "    \"\"\"\n",
        "    Training function compatible with standard GPTModel.\n",
        "    GPTModel.forward(input_ids) returns logits, not a dictionary.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "\n",
        "    totals = {\n",
        "        \"total\": 0.0,\n",
        "        \"cls\": 0.0\n",
        "    }\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass - GPTModel returns logits directly\n",
        "        logits = model(input_ids)  # Shape: [batch, seq_len, vocab_size]\n",
        "\n",
        "        # Use last token for classification\n",
        "        last_token_logits = logits[:, -1, :]  # [batch, vocab_size]\n",
        "\n",
        "        # Classification on top of the last token logits\n",
        "        cls_logits = classifier(last_token_logits)\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "\n",
        "        cls_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        totals[\"total\"] += cls_loss.item()\n",
        "        totals[\"cls\"] += cls_loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in totals.items()}\n"
      ],
      "metadata": {
        "id": "XukpK9zT411N"
      },
      "id": "XukpK9zT411N",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model,\n",
        "    classifier,\n",
        "    loader,\n",
        "    device\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluation function compatible with standard GPTModel.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    cls_losses = []\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(input_ids)  # [batch, seq_len, vocab_size]\n",
        "        last_token_logits = logits[:, -1, :]  # [batch, vocab_size]\n",
        "\n",
        "        # Classification\n",
        "        cls_logits = classifier(last_token_logits)\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "        preds = torch.argmax(cls_logits, dim=-1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        cls_losses.append(cls_loss.item())\n",
        "\n",
        "    # Metrics\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    acc = (all_preds == all_labels).mean()\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=\"binary\", zero_division=0\n",
        "    )\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"cls_loss\": np.mean(cls_losses),\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n"
      ],
      "metadata": {
        "id": "G7CDKsmu43Xl"
      },
      "id": "G7CDKsmu43Xl",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "    model,\n",
        "    classifier,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    alpha=1.0,\n",
        "    beta=0.5,\n",
        "    gamma=0.5\n",
        "):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "\n",
        "    totals = {\n",
        "        \"total\": 0.0,\n",
        "        \"token\": 0.0,\n",
        "        \"sentence\": 0.0,\n",
        "        \"concept\": 0.0,\n",
        "        \"cls\": 0.0\n",
        "    }\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Targets are just input_ids for the auto-regressive part\n",
        "        targets = input_ids.clone()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass returning a DICTIONARY\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "\n",
        "        # Extract components from the dictionary\n",
        "        loss = outputs[\"loss\"]\n",
        "        lt = outputs[\"l_token\"]\n",
        "        ls = outputs[\"l_sentence\"]\n",
        "        lc = outputs[\"l_concept\"]\n",
        "        concept_g = outputs[\"concept_emb\"]\n",
        "\n",
        "        # Classification loss (Spam vs Not Spam) on top of the concept embeddings\n",
        "        cls_logits = classifier(concept_g)\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "\n",
        "        # Combine losses\n",
        "        full_loss = loss + cls_loss\n",
        "\n",
        "        full_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        totals[\"total\"] += full_loss.item()\n",
        "        totals[\"token\"] += lt.item()\n",
        "        totals[\"sentence\"] += ls.item()\n",
        "        totals[\"concept\"] += lc.item()\n",
        "        totals[\"cls\"] += cls_loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in totals.items()}"
      ],
      "metadata": {
        "id": "BMtwIMm65i7-"
      },
      "id": "BMtwIMm65i7-",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model,\n",
        "    classifier,\n",
        "    loader,\n",
        "    device\n",
        "):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    # sent_sims = [] # Calculating this manually is redundant if model does it\n",
        "    token_losses = []\n",
        "    sentence_losses = []\n",
        "\n",
        "    concept_embs = []\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass (returns dict)\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "\n",
        "        # Extract losses directly from model output\n",
        "        token_losses.append(outputs[\"l_token\"].item())\n",
        "        sentence_losses.append(outputs[\"l_sentence\"].item())\n",
        "\n",
        "        concept_g = outputs[\"concept_emb\"]\n",
        "\n",
        "        # Classification\n",
        "        cls_logits = classifier(concept_g)\n",
        "        preds = torch.argmax(cls_logits, dim=-1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        concept_embs.append(concept_g.cpu())\n",
        "\n",
        "    # Metrics\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    acc = (all_preds == all_labels).mean()\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=\"binary\"\n",
        "    )\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Concept separability analysis\n",
        "    concept_embs = torch.cat(concept_embs, dim=0)\n",
        "\n",
        "    # Normalize for cosine similarity\n",
        "    concept_embs = F.normalize(concept_embs, dim=-1)\n",
        "\n",
        "    sim_matrix = torch.matmul(concept_embs, concept_embs.T)\n",
        "\n",
        "    # Create mask for same-label pairs\n",
        "    # Note: This operation is O(N^2) and might be slow for large validation sets\n",
        "    # We'll take a subset if necessary, but for small datasets it's fine.\n",
        "    labels_tensor = torch.tensor(all_labels)\n",
        "    same = labels_tensor[:, None] == labels_tensor[None, :]\n",
        "\n",
        "    # Calculate intra-class (same label) and inter-class (different label) similarity\n",
        "    # We mask out the diagonal (self-similarity) for intra calculation\n",
        "    n = same.shape[0]\n",
        "    diag_mask = ~torch.eye(n, dtype=torch.bool)\n",
        "\n",
        "    intra_mask = same & diag_mask\n",
        "    inter_mask = ~same\n",
        "\n",
        "    intra = sim_matrix[intra_mask].mean().item() if intra_mask.any() else 0.0\n",
        "    inter = sim_matrix[inter_mask].mean().item() if inter_mask.any() else 0.0\n",
        "\n",
        "    return {\n",
        "        \"token_loss\": np.mean(token_losses),\n",
        "        \"sentence_loss\": np.mean(sentence_losses), # Lower is better (1 - sim)\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"concept_intra_similarity\": intra,\n",
        "        \"concept_inter_similarity\": inter\n",
        "    }"
      ],
      "metadata": {
        "id": "g217a5-h5j5g"
      },
      "id": "g217a5-h5j5g",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "spam_training_loop",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spam_training_loop",
        "outputId": "4a0ee1ba-6910-4f91-8d7c-7201bd59d69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STARTING SPAM CLASSIFICATION TRAINING\n",
            "======================================================================\n",
            "\n",
            "Model: GPTModel\n",
            "Device: cuda\n",
            "Train batches: 130\n",
            "Val batches: 19\n",
            "Test batches: 38\n",
            "\n",
            "Training for 5 epochs...\n",
            "======================================================================\n",
            "\n",
            "Epoch 1/5\n",
            "\n",
            "  Train Loss:      0.7117\n",
            "  Val Loss:        0.6847\n",
            "  Val Accuracy:    52.35%\n",
            "  Val Precision:   0.5278\n",
            "  Val Recall:      0.9620\n",
            "  Val F1 Score:    0.6816\n",
            "\n",
            "Epoch 2/5\n",
            "\n",
            "  Train Loss:      0.6709\n",
            "  Val Loss:        0.6652\n",
            "  Val Accuracy:    76.51%\n",
            "  Val Precision:   0.7973\n",
            "  Val Recall:      0.7468\n",
            "  Val F1 Score:    0.7712\n",
            "\n",
            "Epoch 3/5\n",
            "\n",
            "  Train Loss:      0.6418\n",
            "  Val Loss:        0.6524\n",
            "  Val Accuracy:    57.72%\n",
            "  Val Precision:   0.7857\n",
            "  Val Recall:      0.2785\n",
            "  Val F1 Score:    0.4112\n",
            "\n",
            "Epoch 4/5\n",
            "\n",
            "  Train Loss:      0.6205\n",
            "  Val Loss:        0.6433\n",
            "  Val Accuracy:    53.02%\n",
            "  Val Precision:   0.7647\n",
            "  Val Recall:      0.1646\n",
            "  Val F1 Score:    0.2708\n",
            "\n",
            "Epoch 5/5\n",
            "\n",
            "  Train Loss:      0.6026\n",
            "  Val Loss:        0.6409\n",
            "  Val Accuracy:    48.32%\n",
            "  Val Precision:   0.6667\n",
            "  Val Recall:      0.0506\n",
            "  Val F1 Score:    0.0941\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "======================================================================\n",
            "FINAL TEST SET RESULTS\n",
            "======================================================================\n",
            "  Test Loss:       0.6331\n",
            "  Test Accuracy:   49.33%\n",
            "  Test Precision:  0.4000\n",
            "  Test Recall:     0.0132\n",
            "  Test F1 Score:   0.0256\n",
            "\n",
            "Confusion Matrix:\n",
            "[[146   3]\n",
            " [149   2]]\n",
            "======================================================================\n",
            "\n",
            " Saving trained model and classifier...\n",
            " Checkpoint saved to 'spam_classifier_checkpoint.pth'\n",
            "\n",
            " All done!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SPAM CLASSIFICATION TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING SPAM CLASSIFICATION TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model = gpt # Use the loaded GPT2-medium (355M) model\n",
        "model.to(device)\n",
        "\n",
        "# Freeze all parameters of the base GPT model initially\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the original output head with a new classification head\n",
        "# This head will take the embeddings from the final_norm layer (emb_dim) and map to num_classes\n",
        "model.out_head = torch.nn.Linear(in_features=NEW_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
        "model.out_head.to(device) # Move the new head to the correct device\n",
        "\n",
        "# Only the new output head's parameters need gradients\n",
        "for param in model.out_head.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(f\"\\nModel: {model.__class__.__name__}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Redefine train_epoch function to work with the modified model (no external classifier needed)\n",
        "def train_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device\n",
        "):\n",
        "    \"\"\"\n",
        "    Training function compatible with GPTModel where out_head is the classifier.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    totals = {\n",
        "        \"total\": 0.0\n",
        "    }\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass - Model's new out_head directly gives classification logits\n",
        "        logits = model(input_ids)  # Shape: [batch, seq_len, num_classes]\n",
        "\n",
        "        # Use last token's logits for classification loss\n",
        "        cls_logits = logits[:, -1, :] # Shape: [batch, num_classes]\n",
        "\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "\n",
        "        cls_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        totals[\"total\"] += cls_loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in totals.items()}\n",
        "\n",
        "# Redefine evaluate function to work with the modified model (no external classifier needed)\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model,\n",
        "    loader,\n",
        "    device\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluation function compatible with GPTModel where out_head is the classifier.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    cls_losses = []\n",
        "\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(input_ids)  # [batch, seq_len, num_classes]\n",
        "        cls_logits = logits[:, -1, :] # [batch, num_classes]\n",
        "\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels)\n",
        "        preds = torch.argmax(cls_logits, dim=-1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        cls_losses.append(cls_loss.item())\n",
        "\n",
        "    # Metrics\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    acc = (all_preds == all_labels).mean()\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=\"binary\", zero_division=0\n",
        "    )\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"cls_loss\": np.mean(cls_losses),\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "\n",
        "# Training configuration\n",
        "epochs = 5\n",
        "print(f\"\\nTraining for {epochs} epochs...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize optimizer for the trainable parameters (only the new out_head)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=3e-5,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    train_metrics = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Validation phase\n",
        "    val_metrics = evaluate(\n",
        "        model,\n",
        "        val_loader,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    print(f\"{''*70}\")\n",
        "    print(f\"  Train Loss:      {train_metrics['total']:.4f}\")\n",
        "    print(f\"  Val Loss:        {val_metrics['cls_loss']:.4f}\")\n",
        "    print(f\"  Val Accuracy:    {val_metrics['accuracy']*100:.2f}%\")\n",
        "    print(f\"  Val Precision:   {val_metrics['precision']:.4f}\")\n",
        "    print(f\"  Val Recall:      {val_metrics['recall']:.4f}\")\n",
        "    print(f\"  Val F1 Score:    {val_metrics['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Final evaluation on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_metrics = evaluate(\n",
        "    model,\n",
        "    test_loader,\n",
        "    device\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"FINAL TEST SET RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Test Loss:       {test_metrics['cls_loss']:.4f}\")\n",
        "print(f\"  Test Accuracy:   {test_metrics['accuracy']*100:.2f}%\")\n",
        "print(f\"  Test Precision:  {test_metrics['precision']:.4f}\")\n",
        "print(f\"  Test Recall:     {test_metrics['recall']:.4f}\")\n",
        "print(f\"  Test F1 Score:   {test_metrics['f1']:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(test_metrics['confusion_matrix'])\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save the trained model and classifier (optional)\n",
        "print(\"\\n Saving trained model and classifier...\")\n",
        "torch.save({\n",
        "    'epoch': epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'test_accuracy': test_metrics['accuracy'],\n",
        "    'test_f1': test_metrics['f1']\n",
        "}, 'spam_classifier_checkpoint.pth')\n",
        "print(\" Checkpoint saved to 'spam_classifier_checkpoint.pth'\")\n",
        "\n",
        "print(\"\\n All done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "spam_inference_testing",
      "metadata": {
        "id": "spam_inference_testing",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de710d6d-9241-4989-9550-86f18e9c740a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TESTING SPAM CLASSIFIER ON INDIVIDUAL MESSAGES\n",
            "======================================================================\n",
            "\n",
            "1. Message: \"Hi, let's catch up for coffee tomorrow!\"\n",
            "   Prediction: NOT SPAM (68.5% confidence)\n",
            "\n",
            "2. Message: \"URGENT: Your account will be closed! Click here now to verif...\"\n",
            "   Prediction: NOT SPAM (62.9% confidence)\n",
            "\n",
            "3. Message: \"Meeting scheduled for 3pm on Tuesday in conference room B\"\n",
            "   Prediction: NOT SPAM (67.5% confidence)\n",
            "\n",
            "4. Message: \"Congratulations! You've won $1,000,000! Claim your prize now...\"\n",
            "   Prediction: NOT SPAM (66.0% confidence)\n",
            "\n",
            "5. Message: \"Can you send me the report when you get a chance?\"\n",
            "   Prediction: NOT SPAM (66.7% confidence)\n",
            "\n",
            "6. Message: \"FREE MONEY! No purchase necessary! Limited time offer! Act n...\"\n",
            "   Prediction: NOT SPAM (67.1% confidence)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TEST INDIVIDUAL MESSAGES (Inference)\n",
        "# ============================================================================\n",
        "\n",
        "def classify_message(text, model, tokenizer, device, max_length=120, pad_token_id=50256):\n",
        "    \"\"\"Classify a single message as spam or not spam.\n",
        "    The model's out_head is expected to be the classification layer.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    input_ids = tokenizer.encode(text)\n",
        "\n",
        "    # Truncate or pad\n",
        "    if len(input_ids) > max_length:\n",
        "        input_ids = input_ids[:max_length]\n",
        "    else:\n",
        "        input_ids = input_ids + [pad_token_id] * (max_length - len(input_ids))\n",
        "\n",
        "    # Convert to tensor\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        # model(input_tensor) directly returns classification logits due to modified out_head\n",
        "        logits = model(input_tensor)\n",
        "        cls_logits = logits[:, -1, :] # Logits for the last token are the classification scores\n",
        "        prediction = torch.argmax(cls_logits, dim=-1).item()\n",
        "        probabilities = torch.softmax(cls_logits, dim=-1)[0]\n",
        "\n",
        "    label = \"SPAM\" if prediction == 1 else \"NOT SPAM\"\n",
        "    confidence = probabilities[prediction].item() * 100\n",
        "\n",
        "    return label, confidence\n",
        "\n",
        "# Test with example messages\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING SPAM CLASSIFIER ON INDIVIDUAL MESSAGES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_messages = [\n",
        "    \"Hi, let's catch up for coffee tomorrow!\",\n",
        "    \"URGENT: Your account will be closed! Click here now to verify: http://fake-link.com\",\n",
        "    \"Meeting scheduled for 3pm on Tuesday in conference room B\",\n",
        "    \"Congratulations! You've won $1,000,000! Claim your prize now!!!\",\n",
        "    \"Can you send me the report when you get a chance?\",\n",
        "    \"FREE MONEY! No purchase necessary! Limited time offer! Act now!\"\n",
        "]\n",
        "\n",
        "for i, message in enumerate(test_messages, 1):\n",
        "    # Note: 'classifier' argument is removed as the model itself performs classification\n",
        "    label, confidence = classify_message(\n",
        "        message, model, tokenizer, device\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{i}. Message: \\\"{message[:60]}{'...' if len(message) > 60 else ''}\\\"\")\n",
        "    print(f\"   Prediction: {label} ({confidence:.1f}% confidence)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}